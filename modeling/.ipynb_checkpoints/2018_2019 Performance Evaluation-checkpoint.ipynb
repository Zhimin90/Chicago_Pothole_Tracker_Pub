{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "CSV_PATH = \"/home/zhimin90/CPT/CSVs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_in = open(CSV_PATH + \"density_matrix_t_series_2018-2019.pkl\",\"rb\")\n",
    "matrix_test = dill.load(f_in)\n",
    "f_in.close()\n",
    "density_matrix_t_series = matrix_test\n",
    "\n",
    "\n",
    "f_in = open(CSV_PATH + \"grid_matrix_2018-2019.pkl\",\"rb\")\n",
    "matrix_test = dill.load(f_in)\n",
    "f_in.close()\n",
    "grid_matrix = matrix_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = round(len(density_matrix_t_series)*.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "#np.seterr(divide='ignore', invalid='ignore')\n",
    "scaler = StandardScaler()\n",
    "scaler2 = MinMaxScaler()\n",
    "dm_series_np = np.array(density_matrix_t_series[:s])\n",
    "flattened_matrix_np = np.reshape(dm_series_np, (dm_series_np.shape[0]*dm_series_np.shape[1], dm_series_np.shape[1]))\n",
    "\n",
    "normalized_matrices = scaler.fit_transform(flattened_matrix_np)\n",
    "normalized_matrices = scaler2.fit_transform(normalized_matrices)\n",
    "\n",
    "inverseTcheck = scaler2.inverse_transform(normalized_matrices)\n",
    "inverseTcheck = scaler.inverse_transform(inverseTcheck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = normalized_matrices[0:-normalized_matrices.shape[1]].copy()\n",
    "y_train = normalized_matrices[normalized_matrices.shape[1]-1:-1].copy()\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train2 = np.reshape(x_train, (int(x_train.shape[0]/x_train.shape[1]), x_train.shape[1], x_train.shape[1]))\n",
    "x_train2 = np.reshape(x_train, (int(x_train.shape[0]/x_train.shape[1]), x_train.shape[1], x_train.shape[1]))\n",
    "x_train2 = np.reshape(x_train, (x_train2.shape[0] * x_train2.shape[1], 1, x_train2.shape[2]))\n",
    "y_train2 = np.reshape(y_train, (int(y_train.shape[0]/y_train.shape[1]), y_train.shape[1], y_train.shape[1]))\n",
    "y_train2 = np.reshape(y_train, (y_train2.shape[0]*y_train2.shape[1], y_train2.shape[2]))\n",
    "print(x_train2.shape)\n",
    "print(y_train2.shape)\n",
    "x_train2.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_series_np = np.array(density_matrix_t_series[s:])\n",
    "flattened_matrix_np = np.reshape(dm_series_np, (dm_series_np.shape[0]*dm_series_np.shape[1], dm_series_np.shape[2]))\n",
    "\n",
    "normalized_matrices_test = scaler2.transform(scaler.transform(flattened_matrix_np))\n",
    "x_test = normalized_matrices_test[0:-normalized_matrices_test.shape[1]].copy()\n",
    "y_test = normalized_matrices_test[normalized_matrices_test.shape[1]-1:-1].copy()\n",
    "\n",
    "x_test2 = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n",
    "y_test2 = y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.animation as animation \n",
    "import numpy as np \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "import tensorflow.keras as keras\n",
    "import types\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "\n",
    "def make_keras_picklable():\n",
    "    def __getstate__(self):\n",
    "        model_str = \"\"\n",
    "        with tempfile.NamedTemporaryFile(suffix='.hdf5', mode='w', delete=True, dir=os.getcwd()) as fd:\n",
    "            keras.models.save_model(self, fd.name, overwrite=True)\n",
    "            model_str = fd.read()\n",
    "        d = { 'model_str': model_str }\n",
    "        return d\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        with tempfile.NamedTemporaryFile(suffix='.hdf5',mode='r', delete=True, dir=os.getcwd()) as fd:\n",
    "            fd.write(state['model_str'])\n",
    "            fd.flush()\n",
    "            model = keras.models.load_model(fd.name)\n",
    "        self.__dict__ = model.__dict__\n",
    "\n",
    "\n",
    "    cls = keras.models.Model\n",
    "    cls.__getstate__ = __getstate__\n",
    "    cls.__setstate__ = __setstate__\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "make_keras_picklable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(1024, input_shape= (1, x_train2.shape[2]), activation='tanh', return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(1024, activation='tanh', return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(1024, activation='tanh'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(512, activation='tanh'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(x_train2.shape[2], activation='selu'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adamax(learning_rate=0.002, beta_1=0.9, beta_2=0.999)\n",
    "#opt = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
    "#opt = tf.keras.optimizers.Nadam(learning_rate=0.002, beta_1=0.9, beta_2=0.999)\n",
    "#opt = tf.keras.optimizers.Adadelta(learning_rate=1.0, rho=0.95)\n",
    "\n",
    "model.compile(\n",
    "    loss='mean_absolute_error',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "model.fit(x_train2,\n",
    "          y_train2,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Export the model to a SavedModel\n",
    "#model.save(CSV_PATH + 'TensorFlowModel_train_save', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_model = keras.models.load_model(CSV_PATH + 'TensorFlowModel_save')\n",
    "model = keras.models.load_model(CSV_PATH + 'TensorFlowModel_train_save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_test = model.predict(x_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_test = y_test2 - pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(error_test[1,:1]/y_test2[1,:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE = np.mean(abs(error_test/y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pred_test \n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scaler.inverse_transform(scaler2.inverse_transform(pred))\n",
    "print(type(data))\n",
    "data2 = data.reshape((int(data.shape[0]/data.shape[1]), data.shape[1], data.shape[1]))\n",
    "print(data2[-1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_series_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_y_test = dm_series_np[1:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(target_y_test))\n",
    "print(np.min(target_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = np.mean(abs(target_y_test-data2))\n",
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAPE = np.mean(abs((target_y_test-data2)/target_y_test))\n",
    "MAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAPE run into issues with dividing by close to 0 probability density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = np.sqrt(np.mean(np.square(target_y_test-data2)))\n",
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_in2 = open(CSV_PATH + \"map_arr_2018-2019.pkl\",\"rb\")\n",
    "map_arr = dill.load(f_in2)\n",
    "f_in2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "grid_size = 1000\n",
    "# Define the borders\n",
    "x = [-87.9361,-87.5245]\n",
    "y = [41.6447,42.023]\n",
    "deltaX = (max(x) - min(x))/10\n",
    "deltaY = (max(y) - min(y))/10\n",
    "xmin = min(x) - deltaX\n",
    "xmax = max(x) + deltaX\n",
    "ymin = min(y) - deltaY\n",
    "ymax = max(y) + deltaY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = np.mgrid[xmin:xmax:(grid_size*1j), ymin:ymax:(grid_size*1j)]\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.gca()\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "cfset = ax.contourf(xx, yy, data2[-1], cmap='coolwarm')\n",
    "ax.imshow(np.rot90(data2[-1]), cmap='coolwarm', extent=[xmin, xmax, ymin, ymax])\n",
    "cset = ax.contour(xx, yy, density_matrix_t_series[1], colors='k')\n",
    "ax.clabel(cset, inline=1, fontsize=10)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "plt.title('2D Gaussian Kernel density estimation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(map_arr))\n",
    "print(data2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_from_latest = -30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [30, 30]\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.scatter(x=\"LONGITUDE\", y=\"LATITUDE\", data=map_arr[frame_from_latest], c=\"black\", alpha=0.2)\n",
    "ax1.imshow(np.rot90(data2[frame_from_latest]), cmap=plt.cm.twilight_shifted,alpha=0.8, extent=[xmin, xmax, ymin, ymax])\n",
    "#ax1.imshow(np.rot90(target_y_test[frame_from_latest]), cmap=plt.cm.hsv,alpha=0.3, extent=[xmin, xmax, ymin, ymax])\n",
    "ax1.plot(x, y, 'k.', markersize=1)\n",
    "ax1.set_xlim([xmin, xmax])\n",
    "ax1.set_ylim([ymin, ymax])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.rot90(data2).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterable = [np.rot90(matrix) for matrix in data2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterable[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.animation as animation \n",
    "import numpy as np \n",
    "plt.style.use('ggplot')\n",
    "mpl.rcParams['agg.path.chunksize'] = 10000\n",
    "\n",
    "df = map_arr[-1]\n",
    "xdata = df[\"LONGITUDE\"].dropna().to_numpy()\n",
    "ydata = df[\"LATITUDE\"].dropna().to_numpy()\n",
    "xlim=(min(xdata), max(xdata)) \n",
    "ylim=(min(ydata), max(ydata))\n",
    "\n",
    "print(xlim)\n",
    "print(ylim)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [30, 30]\n",
    "fig1, ax1 = plt.subplots()\n",
    "#ax1.scatter(x=\"LONGITUDE\", y=\"LATITUDE\", data=map_arr[frame_from_latest], c=\"red\", alpha=0.4)\n",
    "im = plt.imshow( X=iterable[0], cmap=plt.cm.twilight, extent=[xmin, xmax, ymin, ymax])\n",
    "\n",
    "\n",
    "# initialization function \n",
    "def init(): \n",
    "    # creating an empty plot/frame\n",
    "    # print(\"in init\")\n",
    "    im.set_data(iterable[0])\n",
    "    return [im]\n",
    "\n",
    "\n",
    "# animation function \n",
    "def animate(i):\n",
    "    #df = map_arr[i]\n",
    "    #xdata = df[\"LONGITUDE\"].dropna().to_numpy()\n",
    "    #ydata = df[\"LATITUDE\"].dropna().to_numpy()\n",
    "    #np.rot90(data2[i])\n",
    "    #print(\"in animate\")\n",
    "    im.set_data(iterable[i])\n",
    "    return [im]\n",
    "\n",
    "# setting a title for the plot \n",
    "plt.title('Creating time series pothole map with matplotlib!') \n",
    "# hiding the axis details \n",
    "plt.axis('on') \n",
    "\n",
    "# call the animator\t \n",
    "anim = animation.FuncAnimation(fig1, animate, init_func=init,frames=np.arange(0,np.rot90(data2).shape[1],1), interval=100, blit=True,repeat=False) \n",
    "\n",
    "# save the animation as mp4 video file \n",
    "anim.save(CSV_PATH + 'potholes_zone_2018-2019.gif', writer='imagemagick') \n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(data.shape[0]/data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta, date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def predictor(model, data_in, grid, start_frame_date, end_frame_date, time_shift):\n",
    "    xx, yy = grid\n",
    "    offset = yy.shape[0]\n",
    "    print(\"offset = yy.shape[0]\" + str(offset))\n",
    "    xx = xx.ravel()\n",
    "    yy = yy.ravel()\n",
    "    xdelta = abs(xx[1] - xx[1+offset])\n",
    "    ydelta = abs(yy[0] - yy[1+offset])\n",
    "    print(\"xdelta\"+str(xdelta))\n",
    "    print(\"ydelta\"+str(ydelta))\n",
    "    columns = [ 'start_date', 'end_date', 'poly_coordinate', 'density']\n",
    "    \n",
    "    pred = model.predict(data_in)\n",
    "    data = scaler.inverse_transform(scaler2.inverse_transform(pred))\n",
    "    data_reshaped = data.reshape((int(data.shape[0]/data.shape[1]), data.shape[1], data.shape[1]))\n",
    "    print(data_reshaped.shape)\n",
    "    #each cell is a density estimate from KDE that that has been aggregated by number of potholes over time\n",
    "    #This time interval of density cell is input frame time + timeshift the target frame in the model that has shifted forward by\n",
    "    \n",
    "    row_dict = {'start_date' : None, 'end_date' : None, 'poly_coordinate': None, 'density': 0}\n",
    "    #append = pd.DataFrame(columns=columns)\n",
    "    dict_list = []\n",
    "    for t, matrix in enumerate(data_reshaped):\n",
    "        xy_matrix = np.flip(np.rot90(matrix),0)\n",
    "        print(xy_matrix.shape)\n",
    "        row_dict['start_date'] = pd.to_datetime(start_frame_date) + timedelta(days=(time_shift*(t+1)))\n",
    "        row_dict['end_date'] = pd.to_datetime(end_frame_date) + timedelta(days=(time_shift*(t+1)))\n",
    "        \n",
    "        for i, row in enumerate(xy_matrix):\n",
    "            for j, cell in enumerate(row):\n",
    "                pos_index = i + j*xy_matrix.shape[1]\n",
    "                #generate density cell (square) polycoordinate [[cxmin,cymin],[cxmax, cymin],[cxmin, cymax],[cxmax, cymax]]\n",
    "                row_dict['poly_coordinate'] = [[xx[pos_index],yy[pos_index]],[xx[pos_index]+xdelta,yy[pos_index]],[xx[pos_index]+xdelta,yy[pos_index]+ydelta], [xx[pos_index],yy[pos_index]+ydelta]]\n",
    "                row_dict['density'] = cell\n",
    "                dict_list.append(row_dict.copy())\n",
    "\n",
    "    return pd.DataFrame(dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_frame_date = '2017-11-01 00:00:00'\n",
    "end_frame_date = '2018-11-01 00:00:00'\n",
    "time_shift = 7 #days\n",
    "dataframe = predictor(model,x_train2[-(x_train2.shape[2]+1):-1], (xx, yy), start_frame_date, end_frame_date, time_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.iloc[0:dataframe.shape[0]].to_csv(CSV_PATH + \"2017-11-11_4frames_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe['poly_coordinate'].iloc[0]\n",
    "#dataframe['density'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"C:\\\\Users\\\\Zhimin90\\\\Documents\\\\CPT\\\\Chicago_Pothole_Tracker\\\\CSV\\\\2017-11-11_4frames_test.csv\",converters={\"poly_coordinate\": literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_test.iterrows():\n",
    "        #print(row['poly_coordinate'])\n",
    "        start_date = row['start_date']\n",
    "        end_date = row['end_date']\n",
    "        #print(row['poly_coordinate'])\n",
    "        poly_coordinate = ( ((row['poly_coordinate'][0][0], row['poly_coordinate'][0][1]), \\\n",
    "                             (row['poly_coordinate'][1][0], row['poly_coordinate'][1][1]), \\\n",
    "                             (row['poly_coordinate'][2][0], row['poly_coordinate'][2][1]), \\\n",
    "                             (row['poly_coordinate'][3][0], row['poly_coordinate'][3][1])) )\n",
    "        density = row['density']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_coordinate[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:venv]",
   "language": "python",
   "name": "conda-env-venv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
